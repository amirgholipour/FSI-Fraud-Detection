{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCSP-dbMw88x"
   },
   "source": [
    "# 1. **Develop solution:**  Credit Fraud Detection for financial business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the series, we will train an Machine learning or Deep learning based model (implemented in Keras) in for Anomaly Detection in credit card transaction data. The trained model will be evaluated on pre-labeled and anonymized dataset.\n",
    "\n",
    "Ready? Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.  Install the modeling requirements and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll need to **install some libraries** that are not part of our container image. Normally, **Red Hat OpenShift Data Science** is already taking care of this for you, based on what it detects in the code. **Red Hat OpenShift Data Science** will reinstall all those libraries for you every time you launch the notebook!\n",
    "\n",
    "In case you're using this notebook in a different environment, or just to make sure everything is ready, you can run the following cell to install OpenCV (a library to work with images) and Keras (an abstraction layer over Tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MQmKthrSBCld"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Flask\n",
      "  Downloading Flask-2.1.1-py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 KB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gunicorn\n",
      "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m256.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras==2.8\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m324.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow==2.8\n",
      "  Downloading tensorflow-2.8.0-cp38-cp38-manylinux2010_x86_64.whl (497.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m295.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow_datasets==4.5.2\n",
      "  Downloading tensorflow_datasets-4.5.2-py3-none-any.whl (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m311.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn==1.0.2\n",
      "  Downloading scikit_learn-1.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m281.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (3.4.1)\n",
      "Requirement already satisfied: h5py==2.10.0 in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (2.10.0)\n",
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.8->-r requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.8->-r requirements.txt (line 4)) (1.12.1)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m301.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.8->-r requirements.txt (line 4)) (60.9.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.8->-r requirements.txt (line 4)) (0.23.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.8->-r requirements.txt (line 4)) (3.3.0)\n",
      "Collecting numpy>=1.20\n",
      "  Downloading numpy-1.22.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m287.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.8->-r requirements.txt (line 4)) (12.0.0)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 KB\u001b[0m \u001b[31m326.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.8->-r requirements.txt (line 4)) (3.19.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.8->-r requirements.txt (line 4)) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.8->-r requirements.txt (line 4)) (4.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.8->-r requirements.txt (line 4)) (1.43.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.8->-r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.8->-r requirements.txt (line 4)) (1.1.2)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.8->-r requirements.txt (line 4)) (2.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.8->-r requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.8->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.8->-r requirements.txt (line 4)) (1.1.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.7.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 KB\u001b[0m \u001b[31m209.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow_datasets==4.5.2->-r requirements.txt (line 5)) (2.27.1)\n",
      "Requirement already satisfied: importlib-resources in /opt/app-root/lib/python3.8/site-packages (from tensorflow_datasets==4.5.2->-r requirements.txt (line 5)) (5.4.0)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib/python3.8/site-packages (from tensorflow_datasets==4.5.2->-r requirements.txt (line 5)) (4.62.3)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: dill in /opt/app-root/lib/python3.8/site-packages (from tensorflow_datasets==4.5.2->-r requirements.txt (line 5)) (0.3.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/app-root/lib/python3.8/site-packages (from scikit-learn==1.0.2->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/app-root/lib/python3.8/site-packages (from scikit-learn==1.0.2->-r requirements.txt (line 6)) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/app-root/lib/python3.8/site-packages (from scikit-learn==1.0.2->-r requirements.txt (line 6)) (1.1.0)\n",
      "Requirement already satisfied: click>=8.0 in /opt/app-root/lib/python3.8/site-packages (from Flask->-r requirements.txt (line 1)) (8.0.3)\n",
      "Requirement already satisfied: Werkzeug>=2.0 in /opt/app-root/lib/python3.8/site-packages (from Flask->-r requirements.txt (line 1)) (2.0.2)\n",
      "Collecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting Jinja2>=3.0\n",
      "  Downloading Jinja2-3.1.1-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 KB\u001b[0m \u001b[31m265.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=3.6.0 in /opt/app-root/lib/python3.8/site-packages (from Flask->-r requirements.txt (line 1)) (4.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/app-root/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/app-root/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 7)) (3.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/app-root/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 7)) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/app-root/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 7)) (9.0.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/app-root/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 7)) (1.3.2)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 KB\u001b[0m \u001b[31m281.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/app-root/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow==2.8->-r requirements.txt (line 4)) (0.37.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/app-root/lib/python3.8/site-packages (from importlib-metadata>=3.6.0->Flask->-r requirements.txt (line 1)) (3.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib/python3.8/site-packages (from Jinja2>=3.0->Flask->-r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/app-root/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets==4.5.2->-r requirements.txt (line 5)) (2.0.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/app-root/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets==4.5.2->-r requirements.txt (line 5)) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets==4.5.2->-r requirements.txt (line 5)) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets==4.5.2->-r requirements.txt (line 5)) (2021.10.8)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/app-root/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8->-r requirements.txt (line 4)) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/app-root/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8->-r requirements.txt (line 4)) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/app-root/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8->-r requirements.txt (line 4)) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/app-root/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/app-root/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8->-r requirements.txt (line 4)) (1.35.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow-metadata->tensorflow_datasets==4.5.2->-r requirements.txt (line 5)) (1.54.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/app-root/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8->-r requirements.txt (line 4)) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/app-root/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8->-r requirements.txt (line 4)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/app-root/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8->-r requirements.txt (line 4)) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/app-root/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/app-root/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8->-r requirements.txt (line 4)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/app-root/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8->-r requirements.txt (line 4)) (3.1.1)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=459e42637d1f55155a3787b50385a31cf329894342fa7822c11a7df0cbb7660c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1z6htrp8/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built promise\n",
      "Installing collected packages: tf-estimator-nightly, keras, promise, numpy, Jinja2, itsdangerous, gunicorn, tensorflow-metadata, Flask, tensorflow_datasets, scikit-learn, tensorboard, imbalanced-learn, tensorflow, imblearn\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.7.0\n",
      "    Uninstalling keras-2.7.0:\n",
      "      Successfully uninstalled keras-2.7.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "      Successfully uninstalled numpy-1.19.2\n",
      "  Attempting uninstall: Jinja2\n",
      "    Found existing installation: Jinja2 2.11.3\n",
      "    Uninstalling Jinja2-2.11.3:\n",
      "      Successfully uninstalled Jinja2-2.11.3\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.1\n",
      "    Uninstalling scikit-learn-0.24.1:\n",
      "      Successfully uninstalled scikit-learn-0.24.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.6.0\n",
      "    Uninstalling tensorboard-2.6.0:\n",
      "      Successfully uninstalled tensorboard-2.6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-gpu 2.7.0 requires keras<2.8,>=2.7.0rc0, but you have keras 2.8.0 which is incompatible.\n",
      "elyra-server 2.2.4 requires jinja2<3.0,>=2.11, but you have jinja2 3.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Flask-2.1.1 Jinja2-3.1.1 gunicorn-20.1.0 imbalanced-learn-0.9.0 imblearn-0.0 itsdangerous-2.1.2 keras-2.8.0 numpy-1.22.3 promise-2.3 scikit-learn-1.0.2 tensorboard-2.8.0 tensorflow-2.8.0 tensorflow-metadata-1.7.0 tensorflow_datasets-4.5.2 tf-estimator-nightly-2.8.0.dev2021122109\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.  Importing the needed libraries and packages\n",
    "Of course, we'll need to import various packages. They are either built in the notebook image you are running, or have been installed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQX7R4bhZy5h"
   },
   "outputs": [],
   "source": [
    "import os, sys; sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "\n",
    "from src.dataloading.read_dataset import readData\n",
    "from src.features.data_preprocessing import preprocessData\n",
    "from src.visualization.visualize import visualizeData\n",
    "from src.modules.build_model import buildModel\n",
    "from src.modules.train_model import  trainModel\n",
    "from src.modules.predict_model import predictor\n",
    "from src.hyper_parameters.hps import get_hyper_paras\n",
    "from src.github_commands.git_utils import gitCommands\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Initialize some hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath,BATCH,EPOCHS,model_Type,model_Name, model_dir,refRepoName,sourceRepoName,refRepoDir,sourceRepoDir,scalerPicklePath = get_hyper_paras()\n",
    "dataPath,BATCH,EPOCHS,model_Type,model_Name, model_dir,refRepoName,sourceRepoName,refRepoDir,sourceRepoDir,scalerPicklePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWe0_rQM4JbC"
   },
   "source": [
    "## 1.5. Reading the  Dataset\n",
    "\n",
    "The data set is available on Kaggle for download - https://www.kaggle.com/dalpozz/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = readData(dataPath).readDataFrame()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6.  Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to resize the data to make them ready for feeing to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_org, test, val, train = preprocessData(data,scalerPicklePath).dataPreProcessing()\n",
    "train_org[1].shape, test[1].shape, val[1].shape, train[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7.  Design and compile the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = buildModel(train_data=train, modelType = 'ml').setupModel()\n",
    "clf = buildModel(train_data=train,modelType = 'dl').setupModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8.  Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = trainModel(clf,train_data=train,savePath =model_dir, modelType = 'ml' ).modelTraining()\n",
    "clf = trainModel(clf,train_data =train,val_data=val,modelType='dl',epochs=100,savePath=model_dir).modelTraining()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9.  Test Model based on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor(clf = clf,data=test, modelType='ml').predict()\n",
    "predictor(clf = clf,data=test, modelType='dl').predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10. Update the private git repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=time.localtime(time.time())\n",
    "uploaddate= 'Update with the latest change ' + str(temp[0])+'_'+str(temp[1])+'_'+str(temp[2])+'_'+str(temp[3])+'_'+str(temp[4])\n",
    "\n",
    "gitCommands(repo_dir = sourceRepoDir,repo_name = sourceRepoName,git_email= os.environ['GIT_EMAIL'], git_username = os.environ['GIT_USER_NAME'].lower(), git_token = os.environ['GIT_TOKEN'], commit_message = uploaddate, file_name = '.').gitPush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deploy solution for Credit Fraud Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Push the change to the inference repo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gitCommands(repo_dir = repoDir,repo_name = repoName,git_email= email, git_username = username, git_token = token, commit_message = uploaddate, file_name = '.').gitPush()\n",
    "gitCommands(repo_dir = refRepoDir,repo_name = refRepoName,git_email= os.environ['GIT_EMAIL'], git_username = os.environ['GIT_USER_NAME'].lower(), git_token = os.environ['GIT_TOKEN'], commit_message = uploaddate, file_name = '.').gitPush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Just need to go to **Redhat Openshift Dedicated** to deploy the app :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Thank you for your time!__"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "segmentation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
