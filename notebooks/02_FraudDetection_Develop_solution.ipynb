{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCSP-dbMw88x"
   },
   "source": [
    "# 1. **Develop solution:**  Credit Fraud Detection for financial business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the series, we will train an Machine learning or Deep learning based model (implemented in Keras) in for Anomaly Detection in credit card transaction data. The trained model will be evaluated on pre-labeled and anonymized dataset.\n",
    "\n",
    "Ready? Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.  Install the modeling requirements and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll need to **install some libraries** that are not part of our container image. Normally, **Red Hat OpenShift Data Science** is already taking care of this for you, based on what it detects in the code. **Red Hat OpenShift Data Science** will reinstall all those libraries for you every time you launch the notebook!\n",
    "\n",
    "In case you're using this notebook in a different environment, or just to make sure everything is ready, you can run the following cell to install OpenCV (a library to work with images) and Keras (an abstraction layer over Tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MQmKthrSBCld"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 KB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /opt/app-root/lib/python3.8/site-packages (from imbalanced-learn->imblearn->-r requirements.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/app-root/lib/python3.8/site-packages (from imbalanced-learn->imblearn->-r requirements.txt (line 1)) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/app-root/lib/python3.8/site-packages (from imbalanced-learn->imblearn->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/app-root/lib/python3.8/site-packages (from imbalanced-learn->imblearn->-r requirements.txt (line 1)) (2.2.0)\n",
      "Collecting scikit-learn>=1.0.1\n",
      "  Downloading scikit_learn-1.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m165.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn, imbalanced-learn, imblearn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\n",
      "Successfully installed imbalanced-learn-0.9.0 imblearn-0.0 scikit-learn-1.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.  Importing the needed libraries and packages\n",
    "Of course, we'll need to import various packages. They are either built in the notebook image you are running, or have been installed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YQX7R4bhZy5h"
   },
   "outputs": [],
   "source": [
    "import os, sys; sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "\n",
    "from src.dataloading.read_dataset import readData\n",
    "from src.features.data_preprocessing import preprocessData\n",
    "from src.visualization.visualize import visualizeData\n",
    "from src.modules.build_model import buildModel\n",
    "from src.modules.train_model import  trainModel\n",
    "from src.modules.predict_model import predictor\n",
    "from src.hyper_parameters.hps import get_hyper_paras\n",
    "from src.github_commands.git_utils import gitCommands\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Initialize some hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/opt/app-root/src/FSI-Fraud-Detection-Workshop/data/raw/creditcard.csv',\n",
       " 64,\n",
       " 100,\n",
       " 'ml',\n",
       " 'LogisticRegression',\n",
       " '/opt/app-root/src/FSI-Fraud-Detection-Inference/models/finalized_ml_model.sav',\n",
       " 'FSI-Fraud-Detection-Inference',\n",
       " 'FSI-Fraud-Detection-Workshop',\n",
       " '/opt/app-root/src/FSI-Fraud-Detection-Inference',\n",
       " '/opt/app-root/src/FSI-Fraud-Detection-Workshop',\n",
       " '/opt/app-root/src/FSI-Fraud-Detection-Inference/models/scaler.pkl')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPath,BATCH,EPOCHS,model_Type,model_Name, model_dir,refRepoName,sourceRepoName,refRepoDir,sourceRepoDir,scalerPicklePath = get_hyper_paras()\n",
    "dataPath,BATCH,EPOCHS,model_Type,model_Name, model_dir,refRepoName,sourceRepoName,refRepoDir,sourceRepoDir,scalerPicklePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWe0_rQM4JbC"
   },
   "source": [
    "## 1.5. Reading the  Dataset\n",
    "\n",
    "The data set is available on Kaggle for download - https://www.kaggle.com/dalpozz/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = readData(dataPath).readDataFrame()\n",
    "data.head()\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6.  Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to resize the data to make them ready for feeing to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_org, test, val, train = preprocessData(data,scalerPicklePath).dataPreProcessing()\n",
    "train_org[1].shape, test[1].shape, val[1].shape, train[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7.  Design and compile the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = buildModel(train_data=train, modelType = model_Type).setupModel()\n",
    "# clf = buildModel(train_data=train, modelType = 'ml').setupModel()\n",
    "# clf = buildModel(train_data=train,modelType = 'dl').setupModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8.  Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = trainModel(clf,train_data =train,val_data=val,modelType=model_Type,epochs=100,savePath=model_dir).modelTraining()\n",
    "# clf = trainModel(clf,train_data=train,savePath =model_dir, modelType = 'ml' ).modelTraining()\n",
    "# clf = trainModel(clf,train_data =train,val_data=val,modelType='dl',epochs=100,savePath=model_dir).modelTraining()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9.  Test Model based on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor(clf = clf,data=test, modelType=model_Type).predict()\n",
    "# predictor(clf = clf,data=test, modelType='ml').predict()\n",
    "# predictor(clf = clf,data=test, modelType='dl').predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10. Update the Private\\Public git repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=time.localtime(time.time())\n",
    "uploaddate= 'Update with the latest change ' + str(temp[0])+'_'+str(temp[1])+'_'+str(temp[2])+'_'+str(temp[3])+'_'+str(temp[4])\n",
    "\n",
    "gitCommands(repo_dir = sourceRepoDir,repo_name = sourceRepoName,git_email= os.environ['GIT_EMAIL'], git_username = os.environ['GIT_USER_NAME'].lower(), git_token = os.environ['GIT_TOKEN'], commit_message = uploaddate, file_name = '.').gitPush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deploy solution for Credit Fraud Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Push the change to the inference repo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gitCommands(repo_dir = repoDir,repo_name = repoName,git_email= email, git_username = username, git_token = token, commit_message = uploaddate, file_name = '.').gitPush()\n",
    "gitCommands(repo_dir = refRepoDir,repo_name = refRepoName,git_email= os.environ['GIT_EMAIL'], git_username = os.environ['GIT_USER_NAME'].lower(), git_token = os.environ['GIT_TOKEN'], commit_message = uploaddate, file_name = '.').gitPush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Deploy as an app with seldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys; sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import subprocess\n",
    "\n",
    "current_path= os.path.dirname(os.getcwd())\n",
    "inference_path = current_path.replace('Workshop','Inference')\n",
    "# sys.path.append(inference_path+\"/deploy/\")\n",
    "os.system('python ' +inference_path+\"/deploy/\" +'ocp_deploy.py' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Thank you for your time!__"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "segmentation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
